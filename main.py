import os
import re
import torch
import uvicorn # pip install "fastapi[all]"
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
from typing import List
import csv         # <-- (ì¶”ê°€ 2)
import json        # <-- (ì¶”ê°€ 3)
import io          # <-- (ì¶”ê°€ 4)
import shutil # <-- (ì¶”ê°€ 1) ë¡œì»¬ ì–´ëŒ‘í„° í´ë” ì‚­ì œìš©
import threading

from google.cloud import storage # <-- GCS ë¼ì´ë¸ŒëŸ¬ë¦¬ ìž„í¬íŠ¸
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import PeftModel

# --- âš ï¸ ì¤‘ìš” ì„¤ì • ---
TRAIN_MAX_SEQ_LENGTH = 1024 
SCHEMA_FILE_PATH = "./schema.sql"
BASE_MODEL_NAME = "paust/pko-t5-base"
ADAPTER_DIR = "./sql-lora-adapter" # â¬…ï¸ GCSì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì–´ëŒ‘í„°ê°€ ë®ì–´ì“¸ ë¡œì»¬ ê²½ë¡œ
JSON_OUTPUT_FILE = "./final_training_data.json" 

GCS_BUCKET_NAME = "text2sql-pipeline-bucket" # â¬…ï¸ ë³¸ì¸ì˜ ë²„í‚· ì´ë¦„
GCS_ADAPTER_PREFIX = "adapters/" # â¬…ï¸ GCSì˜ ì–´ëŒ‘í„°ë“¤ì´ ì €ìž¥ëœ ìƒìœ„ í´ë”

# ê¸°íƒ€ ì„¤ì •
DTYPE = torch.float16 
SQL_PREFIX = "SQL ì¿¼ë¦¬ ìƒì„±: "

# --- Pydantic ëª¨ë¸ (ìž…ë ¥ JSON í˜•ì‹ ì •ì˜) ---
class SQLRequest(BaseModel):
    prompt: str
    tables: List[str]

# (ì‹ ê·œ ì¶”ê°€ 2) ë°°í¬ ìš”ì²­ìš© Pydantic ëª¨ë¸
class AdapterDeployRequest(BaseModel):
    adapter_name: str # ì˜ˆ: "adapter-20251105-073830"

# --- FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
app = FastAPI()
model_cache = {}

# (ì‹ ê·œ ì¶”ê°€ 3) 
# VRAMì— ì ‘ê·¼í•˜ëŠ” ìž‘ì—…(ì¶”ë¡ , í•«ìŠ¤ì™‘)ì´ ë™ì‹œì— ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡ ë§‰ëŠ” ìž ê¸ˆìž¥ì¹˜
model_lock = threading.Lock()

# --- í—¬í¼ í•¨ìˆ˜ (verify_model.pyì—ì„œ ê°€ì ¸ì˜´) ---
def read_file_content(filepath):
    """íŒŒì¼ ë‚´ìš©ì„ ì½ì–´ ë¬¸ìžì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"ì˜¤ë¥˜: {filepath} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        raise IOError(f"ì˜¤ë¥˜: {filepath} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ - {e}")

def extract_schemas(full_sql, table_names):
    """
    full_sql ë¬¸ìžì—´ì„ íŒŒì‹±í•˜ì—¬, table_names ëª©ë¡ì— ìžˆëŠ”
    í…Œì´ë¸”ì˜ 'CREATE TABLE ...;' êµ¬ë¬¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    extracted = []
    for table_name in table_names:
        pattern = rf"(CREATE TABLE\s+{re.escape(table_name)}\s*\(.*?;)"
        match = re.search(pattern, full_sql, re.IGNORECASE | re.DOTALL)
        
        if match:
            extracted.append(match.group(1).strip())
        else:
            print(f"--- ê²½ê³ : '{table_name}' í…Œì´ë¸” ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
    return "\n\n".join(extracted)


# --- (ì‹ ê·œ ì¶”ê°€ 1) prepare_training_data.pyì—ì„œ ê°€ì ¸ì˜¨ ìŠ¤í‚¤ë§ˆ íŒŒì„œ ---
def parse_schema(schema_content):
    """
    .sql íŒŒì¼ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ í…Œì´ë¸” ì´ë¦„ê³¼ CREATE TABLE êµ¬ë¬¸ì„ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    schema_dict = {}
    # 'CREATE TABLE'ë¡œ ì‹œìž‘í•´ì„œ ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ëë‚˜ëŠ” ëª¨ë“  ë¸”ë¡ì„ ì°¾ìŠµë‹ˆë‹¤.
    statements = re.findall(r'(CREATE TABLE.*?;)', schema_content, re.DOTALL | re.IGNORECASE)
    
    if not statements:
        print(f"--- ðŸš¨ ê²½ê³ : '{SCHEMA_FILE_PATH}'ì—ì„œ 'CREATE TABLE ... ;' íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    for statement in statements:
        statement = statement.strip()
        # í…Œì´ë¸” ì´ë¦„ ì¶”ì¶œ
        match = re.search(
            r'CREATE TABLE\s+(?:IF NOT EXISTS\s+)?`?\"?(\w+)`?\"?', 
            statement, 
            re.IGNORECASE
        )
        if match:
            table_name = match.group(1)
            schema_dict[table_name] = statement
            
    return schema_dict

def list_gcs_adapters(bucket_name, prefix):
    """GCSì—ì„œ 'adapters/' í´ë” ì•ˆì˜ í•˜ìœ„ í´ë” ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ë””ë²„ê¹… ëª¨ë“œ)"""
    print(f"\n--- [Debug] list_gcs_adapters: ë²„í‚· '{bucket_name}', ì ‘ë‘ì‚¬ '{prefix}' ì¡°íšŒ ì‹œìž‘ ---")
    
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # (ìˆ˜ì •) 'delimiter'ë¥¼ ì œê±°í•˜ê³  ëª¨ë“  blobì„ ë‹¤ ê°€ì ¸ì˜µë‹ˆë‹¤.
        blobs = bucket.list_blobs(prefix=prefix)
        
        found_files = []
        found_folders = set() # setì„ ì‚¬ìš©í•´ ì¤‘ë³µ í´ë” ì´ë¦„ ì œê±°

        # blobs ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ (ê¶Œí•œì´ ì—†ë‹¤ë©´ ì—¬ê¸°ì„œ ì•„ë¬´ê²ƒë„ ë°˜í™˜ë˜ì§€ ì•ŠìŒ)
        for blob in blobs:
            found_files.append(blob.name) # ì˜ˆ: 'adapters/adapter-xxx/file.bin'
            
            # 'adapters/' ì ‘ë‘ì‚¬ë¥¼ ì œê±°í•œ ë‚˜ë¨¸ì§€ ê²½ë¡œë¥¼ ë´…ë‹ˆë‹¤.
            relative_path = blob.name[len(prefix):] # ì˜ˆ: 'adapter-xxx/file.bin'
            
            # ê²½ë¡œì— '/'ê°€ í¬í•¨ë˜ì–´ ìžˆë‹¤ë©´ (ì¦‰, í•˜ìœ„ í´ë”ê°€ ìžˆë‹¤ë©´)
            if '/' in relative_path:
                # ì²« ë²ˆì§¸ '/' ì•žë¶€ë¶„(í´ë” ì´ë¦„)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
                folder_name = relative_path.split('/')[0]
                found_folders.add(folder_name)

        # --- ë¡œê·¸ ì¶œë ¥ ---
        print(f"--- [Debug] ë°œê²¬ëœ ì´ íŒŒì¼ ìˆ˜: {len(found_files)} ---")
        if found_files:
            # ë„ˆë¬´ ë§Žìœ¼ë©´ í„°ë¯¸ë„ì´ ë©ˆì¶”ë¯€ë¡œ ìµœëŒ€ 5ê°œë§Œ ì¶œë ¥
            print(f"--- [Debug] ë°œê²¬ëœ íŒŒì¼ (ìµœëŒ€ 5ê°œ): {found_files[:5]} ---")
        else:
            print(f"--- [Debug] '{prefix}'ë¡œ ì‹œìž‘í•˜ëŠ” íŒŒì¼ì„ GCSì—ì„œ 'í•˜ë‚˜ë„' ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ---")
            
        print(f"--- [Debug] ì¶”ì¶œëœ í´ë” (set): {found_folders} ---")
        # -----------------
        
        return list(found_folders) # setì„ listë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        
    except Exception as e:
        print(f"--- âŒ GCS ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ (ì˜ˆì™¸ ë°œìƒ): {e} ---")
        return None

def download_gcs_directory(bucket_name, gcs_prefix, local_dir):
    """GCSì˜ íŠ¹ì • í´ë”(prefix)ë¥¼ ë¡œì»¬ ë””ë ‰í† ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œ (ë®ì–´ì“°ê¸°)"""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blobs = bucket.list_blobs(prefix=gcs_prefix) # ì´ í´ë” ì•ˆì˜ ëª¨ë“  íŒŒì¼

        # 1. ê¸°ì¡´ ë¡œì»¬ ì–´ëŒ‘í„° í´ë”ë¥¼ ê¹¨ë—í•˜ê²Œ ì‚­ì œ
        if os.path.exists(local_dir):
            shutil.rmtree(local_dir)
        
        # 2. ë¹ˆ í´ë” ë‹¤ì‹œ ìƒì„±
        os.makedirs(local_dir, exist_ok=True)
        
        download_count = 0
        for blob in blobs:
            # GCS ê²½ë¡œì—ì„œ ë¡œì»¬ ê²½ë¡œë¡œ ë³€í™˜
            relative_path = os.path.relpath(blob.name, gcs_prefix)
            local_path = os.path.join(local_dir, relative_path)
            
            # íŒŒì¼ì´ ì†í•œ í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            blob.download_to_filename(local_path)
            download_count += 1
            
        return download_count
    except Exception as e:
        print(f"--- âŒ GCS ì–´ëŒ‘í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e} ---")
        return None

# --- 1. ì„œë²„ ì‹œìž‘ ì‹œ ëª¨ë¸ ë¡œë“œ ì´ë²¤íŠ¸ ---
@app.on_event("startup")
async def load_model_and_schema():
    """
    FastAPI ì„œë²„ê°€ ì‹œìž‘ë  ë•Œ ëª¨ë¸, í† í¬ë‚˜ì´ì €, 
    ê·¸ë¦¬ê³  CSV ë³€í™˜ì„ ìœ„í•œ 'ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬'ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print("--- FastAPI ì„œë²„ ì‹œìž‘ ---")
    
    try:
        print(f"1. ì „ì²´ ìŠ¤í‚¤ë§ˆ ë¡œë“œ: {SCHEMA_FILE_PATH}")
        full_schema = read_file_content(SCHEMA_FILE_PATH)
        model_cache["full_schema_sql"] = full_schema
        
        # --- (ì‹ ê·œ ì¶”ê°€ 2) ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ íŒŒì‹±í•˜ì—¬ ìºì‹œ ---
        print("2. ìŠ¤í‚¤ë§ˆ íŒŒì‹± (Dict ìƒì„±)...")
        model_cache["schema_dict"] = parse_schema(full_schema)
        # -----------------------------------------------

        # (ì‹ ê·œ ì¶”ê°€ 4) í•«ìŠ¤ì™‘ì„ ìœ„í•´ VRAM ë¡œë“œ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
        print("3. VRAMì— ëª¨ë¸ í•«ìŠ¤ì™‘ ë¡œë“œ ì‹œë„...")
        load_model_into_vram()
        
        print("--- ðŸš€ ëª¨ë¸, ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ ì™„ë£Œ. ì„œë²„ ì¤€ë¹„ë¨. ---")
        
    except FileNotFoundError as e:
        print(f"--- ðŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: {e} ---")
        print("--- 'sql-lora-adapter'ê°€ ì¡´ìž¬í•˜ì§€ ì•Šê±°ë‚˜ 'schema.sql' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ---")

# (ì‹ ê·œ ì¶”ê°€ 5) VRAMì— ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•µì‹¬ ë¡œì§ (ìž¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë¶„ë¦¬)
def load_model_into_vram():
    """
    ë¡œì»¬ ADAPTER_DIRì˜ ì–´ëŒ‘í„°ë¥¼ VRAMìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    (ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” model_lockìœ¼ë¡œ ë³´í˜¸ëœ ìƒíƒœì—ì„œ í˜¸ì¶œë˜ì–´ì•¼ í•¨)
    """
    try:
        # 1. ê¸°ì¡´ ëª¨ë¸ì´ VRAMì— ìžˆë‹¤ë©´ ë¹„ìš°ê¸° (í•«ìŠ¤ì™‘)
        if "model" in model_cache:
            print("--- [Hot-Swap] ê¸°ì¡´ ëª¨ë¸ VRAMì—ì„œ ì œê±° ì‹œë„... ---")
            del model_cache["model"]
            del model_cache["tokenizer"]
            torch.cuda.empty_cache() # VRAM ì°Œêº¼ê¸° ì²­ì†Œ
            print("--- [Hot-Swap] VRAM ì œê±° ì™„ë£Œ. ---")

        # 2. ìƒˆ íŒŒì¼ë¡œ í† í¬ë‚˜ì´ì €/ëª¨ë¸ ë¡œë“œ
        print(f"--- [Hot-Swap] VRAM ë¡œë“œ ì‹œìž‘: {ADAPTER_DIR} ---")
        
        # (startupê³¼ ë™ì¼í•œ ë¡œì§)
        tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)
        
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
        base_model = AutoModelForSeq2SeqLM.from_pretrained(
            BASE_MODEL_NAME, quantization_config=bnb_config,
            dtype=DTYPE, device_map="auto",
        )
        model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)
        model.eval()
        
        # 3. ìºì‹œì— ì €ìž¥
        model_cache["tokenizer"] = tokenizer
        model_cache["model"] = model
        print("--- [Hot-Swap] VRAM ë¡œë“œ ì™„ë£Œ. ---")
        
    except Exception as e:
        print(f"--- ðŸš¨ í•«ìŠ¤ì™‘ ì¤‘ VRAM ë¡œë“œ ì‹¤íŒ¨: {e} ---")
        # ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•˜ë©´ ì„œë²„ê°€ ì‘ë‹µ ë¶ˆê°€ëŠ¥ ìƒíƒœê°€ ë˜ë¯€ë¡œ,
        # model_cacheë¥¼ ë¹„ì›Œ /verify-modelì´ ì‹¤íŒ¨í•˜ë„ë¡ ìœ ë„
        model_cache.pop("model", None)
        model_cache.pop("tokenizer", None)
        raise e # ìƒìœ„ í•¸ë“¤ëŸ¬ê°€ ì˜¤ë¥˜ë¥¼ ìž¡ë„ë¡ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´

# --- (ì‹ ê·œ ì¶”ê°€ 4) GCS ì–´ëŒ‘í„° ëª©ë¡ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/list-adapters")
async def get_gcs_adapters():
    """GCS ë²„í‚·ì˜ 'adapters/' í´ë”ì— ì €ìž¥ëœ ì–´ëŒ‘í„° ë²„ì „ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    adapters = list_gcs_adapters(GCS_BUCKET_NAME, GCS_ADAPTER_PREFIX)
    if adapters is None:
        raise HTTPException(status_code=500, detail="GCSì—ì„œ ì–´ëŒ‘í„° ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    return {"adapters": adapters}

# --- (ì‹ ê·œ ì¶”ê°€ 5) ì–´ëŒ‘í„° ë°°í¬(ë‹¤ìš´ë¡œë“œ/ë®ì–´ì“°ê¸°) ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/deploy-adapter")
async def deploy_adapter_from_gcs(request: AdapterDeployRequest):
    """
    GCSì—ì„œ ì§€ì •ëœ ì–´ëŒ‘í„°ë¥¼ ë¡œì»¬ 'ADAPTER_DIR'ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë®ì–´ì”ë‹ˆë‹¤.
    (ì£¼ì˜: ì´ ìž‘ì—… í›„ ì„œë²„ë¥¼ 'ìˆ˜ë™ìœ¼ë¡œ ìž¬ì‹œìž‘'í•´ì•¼ ì ìš©ë©ë‹ˆë‹¤.)
    """
    adapter_name = request.adapter_name
    gcs_prefix = f"{GCS_ADAPTER_PREFIX.strip('/')}/{adapter_name}" # ì˜ˆ: adapters/adapter-xxx

    # (ì‹ ê·œ ì¶”ê°€ 6) ë‹¤ë¥¸ ìš”ì²­(verify)ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§‰ìŒ
    if not model_lock.acquire(timeout=5): # 5ì´ˆê°„ ìž ê¸ˆì„ ëª» ì–»ìœ¼ë©´ í¬ê¸°
         raise HTTPException(status_code=503, detail="ì„œë²„ê°€ ë‹¤ë¥¸ ìž‘ì—…(ë°°í¬/ì¶”ë¡ )ìœ¼ë¡œ ë°”ì©ë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
         
    try:
        print(f"\n--- [Deploy] í•«ìŠ¤ì™‘ ë°°í¬ ì‹œìž‘: {adapter_name} ---")
        
        # 1. íŒŒì¼ ë‹¤ìš´ë¡œë“œ (VRAM ì‚¬ìš© ì•ˆ í•¨)
        count = download_gcs_directory(GCS_BUCKET_NAME, gcs_prefix, ADAPTER_DIR)
        
        if count is None:
            raise HTTPException(status_code=500, detail=f"'{adapter_name}' ë‹¤ìš´ë¡œë“œ ì¤‘ GCS ì˜¤ë¥˜ ë°œìƒ")
        if count == 0:
            raise HTTPException(status_code=404, detail=f"'{adapter_name}'ì„ GCSì—ì„œ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"--- [Deploy] GCS ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. VRAM í•«ìŠ¤ì™‘ ì‹œìž‘ (ì„œë²„ ë©ˆì¶¤) ---")

        # 2. (í•µì‹¬) VRAM í•«ìŠ¤ì™‘ ì‹¤í–‰
        # ì´ ìž‘ì—…ì€ 10~20ì´ˆê°„ ë¸”ë¡œí‚¹ë©ë‹ˆë‹¤.
        load_model_into_vram() 
        
        message = f"ì„±ê³µ: '{adapter_name}'ì„ ë°°í¬í•˜ê³  VRAMì— í•«ìŠ¤ì™‘ ì™„ë£Œ. ì„œë²„ê°€ ìƒˆ ëª¨ë¸ë¡œ ì¦‰ì‹œ ì‘ë‹µí•©ë‹ˆë‹¤."
        print(message)
        return {"message": message, "deployed_adapter": adapter_name}
        
    except Exception as e:
        # í•«ìŠ¤ì™‘ ì¤‘ ì˜¤ë¥˜ê°€ ë‚˜ë©´ 500 ì—ëŸ¬ ë°˜í™˜
        raise HTTPException(status_code=500, detail=f"ë°°í¬ ë° í•«ìŠ¤ì™‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
    finally:
        # (ì‹ ê·œ ì¶”ê°€ 7) ìž‘ì—…ì´ ëë‚˜ë©´ (ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“ ) ìž ê¸ˆ í•´ì œ
        model_lock.release()
        print("--- [Deploy] ìž ê¸ˆ í•´ì œ. ---")

@app.post("/upload-dataset")
async def create_training_data_from_csv(file: UploadFile = File(...)):
    """
    CSV íŒŒì¼ì„ ì—…ë¡œë“œë°›ì•„ 'prepare_training_data.py' ë¡œì§ì„ ìˆ˜í–‰í•˜ê³ 
    ì„œë²„ ë¡œì»¬ì— 'final_training_data.json' íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.
    """
    # 0. í›ˆë ¨ ì¤‘ì—ëŠ” ì´ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© ë¶ˆê°€ (VRAM ì¶©ëŒ ë°©ì§€)
    # (ë‚˜ì¤‘ì—ëŠ” ìƒíƒœ ê´€ë¦¬ê°€ í•„ìš”í•˜ì§€ë§Œ, ì§€ê¸ˆì€ 'ìˆ˜ë™ ì „í™˜'ì„ ì‹ ë¢°í•©ë‹ˆë‹¤.)
    
    # 1. íŒŒì¼ í˜•ì‹ ê²€ì‚¬
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="CSV íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")

    # 2. ìºì‹œëœ ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ ê°€ì ¸ì˜¤ê¸°
    schema_dict = model_cache.get("schema_dict")
    if not schema_dict:
        raise HTTPException(status_code=500, detail="ì„œë²„ì— ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ìž¬ì‹œìž‘í•˜ì„¸ìš”.")

    print(f"\n--- CSV íŒŒì¼ ìˆ˜ì‹ : {file.filename} ---")
    processed_entries = []
    error_logs = []
    
    try:
        # 3. ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì½ê¸°
        contents = await file.read()
        decoded_content = contents.decode('utf-8')
        csv_file = io.StringIO(decoded_content)
        csv_reader = csv.reader(csv_file)

        # 4. prepare_training_data.py ë¡œì§ ìˆ˜í–‰
        try:
            header = next(csv_reader)
        except StopIteration:
            raise HTTPException(status_code=400, detail="CSV íŒŒì¼ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.")
            
        for i, row in enumerate(csv_reader, start=2):
            try:
                if not row or len(row) < 4:
                    continue # ë¹ˆ ì¤„ì´ë‚˜ ì§§ì€ ì¤„ ê±´ë„ˆë›°ê¸°
                    
                status = row[0].strip()
                
                # "ê²€í† ì™„ë£Œ" í•­ëª©ë§Œ ì²˜ë¦¬
                if status == "ê²€í† ì™„ë£Œ":
                    question = row[1].strip()
                    sql_response = row[2].strip()
                    table_names = [name.strip() for name in row[3:] if name.strip()]
                    
                    schema_parts = []
                    missing_tables = False
                    for table_name in table_names:
                        if table_name in schema_dict:
                            schema_parts.append(schema_dict[table_name])
                        else:
                            missing_tables = True
                            log = f"ê²½ê³ : {i}í–‰ - í…Œì´ë¸” '{table_name}'ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            print(log)
                            error_logs.append(log)
                    
                    schema_string = "\n".join(schema_parts)
                    if not schema_parts and missing_tables:
                         schema_string = "### ERROR: Referenced schemas not found ###"
                    
                    instruction = f"### Schema:\n{schema_string}\n\n### Question:\n{question}"
                    final_sql_response = sql_response.replace("{lang}", "#{lang}")
                    
                    data_entry = {
                        "instruction": instruction,
                        "response": final_sql_response
                    }
                    processed_entries.append(data_entry)
                    
            except Exception as e:
                log = f"ê²½ê³ : CSV {i}í–‰ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}"
                print(log)
                error_logs.append(log)

    except Exception as e:
        print(f"--- âŒ CSV íŒŒì¼ íŒŒì‹± ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e} ---")
        raise HTTPException(status_code=500, detail=f"CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    # 2. (ìˆ˜ì •) ë¡œì»¬ ì €ìž¥ì„ GCS ì—…ë¡œë“œë¡œ ë³€ê²½
    if not processed_entries:
        return {"message": "ì²˜ë¦¬í•  'ê²€í† ì™„ë£Œ' í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.", "processed_count": 0}

    try:
        # JSONL ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ìƒì˜ ë¬¸ìžì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
        jsonl_content = "\n".join([json.dumps(entry, ensure_ascii=False) for entry in processed_entries])
        
        # GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (VM ì„œë¹„ìŠ¤ ê³„ì •ìœ¼ë¡œ ìžë™ ì¸ì¦)
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(GCS_JSON_PATH)
        
        # GCSë¡œ ë¬¸ìžì—´ ì—…ë¡œë“œ
        blob.upload_from_string(jsonl_content, content_type='application/jsonl')
        
        print(f"--- GCS ì—…ë¡œë“œ ì™„ë£Œ: {GCS_JSON_PATH} ---")

    except Exception as e:
        print(f"--- âŒ GCS ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e} ---")
        raise HTTPException(status_code=500, detail=f"GCS íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    return {
        "message": f"ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(processed_entries)}ê°œ í•­ëª©ì„ GCS('{GCS_JSON_PATH}')ì— ì €ìž¥í–ˆìŠµë‹ˆë‹¤.",
        "gcs_path": f"gs://{GCS_BUCKET_NAME}/{GCS_JSON_PATH}",
        "processed_count": len(processed_entries),
        "errors": error_logs
    }


# --- 2. SQL ìƒì„± ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/verify-model")
async def verify_sql_generation(request: SQLRequest):
    # (ì‹ ê·œ ì¶”ê°€ 8) ë°°í¬ ìž‘ì—…ì´ ëª¨ë¸ì„ êµì²´í•˜ëŠ” ì¤‘ì´ë©´ ëŒ€ê¸°
    if not model_lock.acquire(timeout=5):
        raise HTTPException(status_code=503, detail="ì„œë²„ê°€ ëª¨ë¸ ë°°í¬ ìž‘ì—…ìœ¼ë¡œ ë°”ì©ë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        
    try:
        if "model" not in model_cache or "tokenizer" not in model_cache:
            raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ VRAMì— ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. /deploy-adapterë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ê±°ë‚˜ ì„œë²„ë¥¼ ìž¬ì‹œìž‘í•˜ì„¸ìš”.")

        tokenizer = model_cache["tokenizer"]
        model = model_cache["model"]
        full_schema = model_cache["full_schema_sql"]
        
        # ... (ì´í•˜ ì¶”ë¡  ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
        print(f"\n--- ìš”ì²­ ìˆ˜ì‹ : {request.prompt} (í…Œì´ë¸”: {request.tables}) ---")
        filtered_schema = extract_schemas(full_schema, request.tables)
        if not filtered_schema:
            raise HTTPException(status_code=400, detail=f"ìž…ë ¥ëœ í…Œì´ë¸”({request.tables}) ì¤‘ ìœ íš¨í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        input_text = (f"{SQL_PREFIX}### Schema:\n{filtered_schema}\n\n### Question:\n{request.prompt}")
        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=TRAIN_MAX_SEQ_LENGTH).to("cuda")

        with torch.no_grad():
            outputs = model.generate(
                **inputs, max_length=512, num_beams=5, early_stopping=True,
                eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id
            )
        generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {"generated_sql": generated_sql}

    except Exception as e:
        print(f"--- âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
        raise HTTPException(status_code=500, detail=f"SQL ìƒì„± ì¤‘ ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    finally:
        # (ì‹ ê·œ ì¶”ê°€ 9) ì¶”ë¡ ì´ ëë‚˜ë©´ ìž ê¸ˆ í•´ì œ
        model_lock.release()

# --- 3. ì„œë²„ ì‹¤í–‰ (í„°ë¯¸ë„ì—ì„œ uvicornìœ¼ë¡œ ì§ì ‘ ì‹¤í–‰) ---
if __name__ == "__main__":
    # ì´ íŒŒì¼(`main.py`)ì´ ìžˆëŠ” ë””ë ‰í† ë¦¬ì—ì„œ
    # 'uvicorn main:app --host 0.0.0.0 --port 8000' ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”.
    
    # âš ï¸ TRAIN_MAX_SEQ_LENGTH ê°’ì„ í›ˆë ¨ ì‹œ ì„¤ì •ê³¼
    #    ë™ì¼í•˜ê²Œ ë§žì·„ëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ì„¸ìš”. (ê¸°ë³¸ê°’: 1024)
    
    print("--- ì„œë²„ë¥¼ ì‹œìž‘í•˜ë ¤ë©´ í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”: ---")
    print("uvicorn main:app --host 0.0.0.0 --port 8000")
    print("--- --------------------------------------------- ---")