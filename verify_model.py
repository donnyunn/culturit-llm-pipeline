import torch
import os
from transformers import AutoTokenizer, BartForConditionalGeneration
from datasets import Dataset
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
import json

# --- 1. ì„¤ì • ë° ê²½ë¡œ ---
# ìµœì¢… í•™ìŠµ ê²°ê³¼ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ (train_model.pyì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤)
MODEL_PATH = './kobart_model_output/final_model'

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ (docker run ì‹œ ì„¤ì •í–ˆë˜ ê°’)
DB_CONFIG = {
    'DB_NAME': 'llm_schema_db',
    'DB_USER': 'llm_user',
    'DB_PASS': '1a2a3a4a',  # ì„¤ì •í•œ ë¹„ë°€ë²ˆí˜¸ë¡œ ë³€ê²½
    'DB_HOST': '127.0.0.1', # VM ë‚´ë¶€ì—ì„œ docker containerë¡œ ì ‘ê·¼ (PostgreSQL í¬íŠ¸)
    'DB_PORT': '5432'
}

# ìµœì¢… í•™ìŠµ ë°ì´í„°ì…‹ íŒŒì¼
DATA_PATH = './final_training_data.json'

# --- 2. DB ì—°ê²° ë° ì‹¤í–‰ í•¨ìˆ˜ ---
def execute_sql(sql_query):
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì ‘ì†í•˜ì—¬ SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    conn = None
    try:
        # SQLAlchemy ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ì—°ê²° (Pandas read_sql ì‚¬ìš© ëª©ì )
        engine = create_engine(
            f"postgresql+psycopg2://{DB_CONFIG['DB_USER']}:{DB_CONFIG['DB_PASS']}@{DB_CONFIG['DB_HOST']}:{DB_CONFIG['DB_PORT']}/{DB_CONFIG['DB_NAME']}"
        )
        
        # SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°›ìŒ
        df_result = pd.read_sql(sql_query, engine)
        
        return df_result.to_string(index=False, header=True)
        
    except Exception as e:
        return f"âŒ SQL ì‹¤í–‰ ì˜¤ë¥˜: {e}"
    finally:
        if conn:
            conn.close()

# --- 3. ëª¨ë¸ ì¶”ë¡  í•¨ìˆ˜ ---
def generate_sql(model, tokenizer, question, schema_encoding):
    """ì§ˆë¬¸ê³¼ ìŠ¤í‚¤ë§ˆë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ëª¨ë¸ ì…ë ¥ í˜•ì‹: ì§ˆë¬¸ [SEP] ìŠ¤í‚¤ë§ˆ
    input_text = f"{question} [SEP] {schema_encoding}"
    
    # í† í°í™”
    inputs = tokenizer(
        input_text, 
        return_tensors="pt", 
        max_length=1024, 
        truncation=True,
        padding="max_length"
    ).to(model.device)

    # SQL ìƒì„± (ì¶”ë¡ )
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_length=1024,
            num_beams=4,
            early_stopping=True
        )

    # í† í°ì„ SQL í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_sql

# --- 4. ë©”ì¸ ê²€ì¦ ë¡œì§ ---
if __name__ == '__main__':
    print("=========================================")
    print("ğŸ¤– Text to SQL ëª¨ë¸ ê²€ì¦ ì‹œì‘ ğŸ¤–")
    print("=========================================")
    
    # 0. GPU ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    # 1. ëª¨ë¸ ë¡œë“œ
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        # AutoModelForSeq2SeqLM ëŒ€ì‹  BartForConditionalGeneration ì‚¬ìš©
        model = BartForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)
        print(f"âœ… Model loaded successfully from {MODEL_PATH}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        exit()

    # 2. ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë° ìŠ¤í‚¤ë§ˆ ë¡œë“œ
    try:
        # final_training_data.json íŒŒì¼ì—ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ë¡œë“œ
        df = pd.read_json(DATA_PATH, lines=True)
        schema_encoding = df['SCHEMA_ENCODING'].iloc[0] # ëª¨ë“  í–‰ì˜ ìŠ¤í‚¤ë§ˆëŠ” ë™ì¼
        print(f"âœ… Data and Schema loaded. Total {len(df)} test cases.")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit()

    # 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜ (ìƒìœ„ 5ê°œ ë° íŠ¹ì • ì¼€ì´ìŠ¤)
    test_cases = [
        ("ì´ ì§ì›ìˆ˜ê°€ ëª‡ëª…ì´ì•¼?"), # ë‹¨ìˆœ COUNT
        ("ê°œë°œíŒ€(org_code : OC001) ì†Œì† ì§ì›ì˜ ì´ë¦„ê³¼ ì´ë©”ì¼ì„ ì•Œë ¤ì¤˜."), # JOIN ë° WHERE ì¡°ê±´
        ("í™ê¸¸ë™(mem_id : MEM00019)ì˜ ì§ì±…ì€ ë­ì•¼?"), # ì½”ë“œ í…Œì´ë¸” JOIN
        ("ì§€ë‚œ ë²ˆì— ë°˜ë ¤ëœ ê²°ì¬ ë¬¸ì„œë¥¼ ì°¾ì•„ì¤˜."), # ê²°ì¬ ìƒíƒœ ì½”ë“œ ì¡°ê±´
        ("ë‘ ê°œ ì´ìƒì˜ ë¶€ì„œì— ì†Œì†ëœ ì§ì›ì´ ìˆëŠ”ì§€ ì°¾ì•„ì¤˜."), # Subquery ë˜ëŠ” HAVING
    ]

    print("\n=========================================")
    print("ğŸ“Š SQL ìƒì„± ë° DB ì‹¤í–‰ ê²°ê³¼")
    print("=========================================")

    for i, question in enumerate(test_cases):
        print(f"\n--- TEST CASE {i+1} ---")
        print(f"Q: {question}")
        
        # 3.1 SQL ìƒì„± (ì¶”ë¡ )
        generated_sql = generate_sql(model, tokenizer, question, schema_encoding)
        print(f"A: [Generated SQL]\n {generated_sql}")
        
        # 3.2 DB ì‹¤í–‰
        if generated_sql.upper().startswith("SELECT"):
            result_df = execute_sql(generated_sql)
            print(f"R: [DB Result]\n{result_df}")
        else:
            print("R: [DB Result] ìœ íš¨í•˜ì§€ ì•Šì€ SQL í˜•ì‹ì…ë‹ˆë‹¤.")